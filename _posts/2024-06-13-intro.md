---
title: "Introduction to Kubeflow in Vertex AI"
date: 2024-06-13
author: Jose L. Ortiz-Tarin
---

Leveraging the capabilities of machine learning (ML) at scale comes with many operational challenges. In the journey from prototyping a model to deploying it into production, there are many complexities that have to be addressed: reproducibility and metadata management, version control, scalability, model monitoring... Some of these challenges are shared with other disciplines like software development or data engineering but others are very particular to the ML lifecycle. To streamline this process the practice of ML operations (MLOps) as a set of guiding principles and best practices has emerged. One of the core concepts of MLOps is the use of pipelines, which automate and orchestrate the various stages of an ML workflow. 
By using pipelines, the work of a data scientist or an ML engineer becomes more structured and reproducible. This might not be necessary in the initial stages of prototyping but, as the training of new models becomes more computationally expensive, more experiments are done and more metadata is generated, having a centralized system to track executions, source code and results becomes essential. Without it, even reproducing results becomes challenging. Once the models are ready, the production workflow has to be consolidated and the automatic quality control measures have to be put in place. This process of orchestrating the deployment, where traceability is crucial, also requires using pipelines. 
In this series of short articles, we’ll describe how to build pipelines in Google Cloud using Vertex AI and Kubeflow (KFP), an open-source platform for ML in Kubernetes. We’ll pay special attention to concepts that are not covered in the KFP documentation and that are essential to build robust pipelines in GCP, we’ll also give you tips that will accelerate your pipeline development. Generally, there are other adjacent systems that live around the pipelines, e.g a model registry, a container registry, a datamart or a feature store. We’ll also discuss how they can be well integrated into your KFP pipelines.
